{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This notebook is to analyze the search results and the predicted path results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Search results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.1 analyze the completeness of the search results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# each query should get 100 results\n",
    "import os\n",
    "# change the working directory to the root of the project\n",
    "os.chdir(\"/home/aiops/zhuty/tinyllama/processing/graphs\")\n",
    "import tqdm\n",
    "import argparse\n",
    "from utils import read_trec_results\n",
    "version = \"20b\"\n",
    "result_dir = f\"/home/aiops/zhuty/ret_pretraining_data/redpajama_{version}_id_added/bm25_search_results/\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEST = False\n",
    "# read result from all chunks\n",
    "result_dict = {}\n",
    "for i in tqdm.tqdm(list(range(0, 89)) + ['search_fail_queries_added'] + ['missing_queries']):\n",
    "    if TEST and i > 10:\n",
    "        break\n",
    "    file_path = os.path.join(result_dir, \"chunk_{i}.result.txt\".format(i=i))\n",
    "    result_dict.update(read_trec_results(file_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "# Print the results\n",
    "for query_id, docs in result_dict.items():\n",
    "    print(f\"Query {query_id}:\")\n",
    "    count += 1\n",
    "    for doc in docs[:10]:\n",
    "        print(f\"  Doc ID: {doc['doc_id']}, Score: {doc['score']}, Rank: {doc['rank']}\")\n",
    "    if  count > 10:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Testing the completeness of the search results, each query should get 100 results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# count how many docs have itself as the top1 neighbor\n",
    "count = 0\n",
    "result_lengths = []\n",
    "problematic_queries = []\n",
    "for query_id, docs in result_dict.items():\n",
    "    if docs[0]['doc_id'] == query_id:\n",
    "        count += 1\n",
    "    result_lengths.append(len(docs))\n",
    "    if len(docs) < 100:\n",
    "        problematic_queries.append(query_id)\n",
    "print(\"Number of queries that have itself as the top1 neighbor:\", count, f\"percentage: {count/len(result_dict) *100:.2f}%\")\n",
    "print(Counter(result_lengths))\n",
    "print(\"Number of queries that have less than 100 results:\", len(problematic_queries), f\"percentage: {len(problematic_queries)/len(result_dict) *100:.2f}%\")\n",
    "problematic_queries[:10]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sequence_set = set()\n",
    "for i in range(0, 89):\n",
    "    for j in range(0, 10000):\n",
    "        sequence_set.add(\"{i}_{j}\".format(i=i, j=j))\n",
    "\n",
    "missing_queries = []\n",
    "for query_id in sequence_set:\n",
    "    if query_id not in result_dict:\n",
    "        missing_queries.append(query_id)\n",
    "# missing queries is problematic ==> it means that it cannot even search itself ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Number of queries that are missing:\", len(missing_queries), f\"percentage: {len(missing_queries)/len(sequence_set) *100:.2f}%\")\n",
    "print(\"missing queries:\", missing_queries[:10])\n",
    "newly_added_search_fail_queries = set(list(problematic_queries) + missing_queries)\n",
    "print(\"Number of newly added search fail queries:\", len(newly_added_search_fail_queries))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "def get_content(query_id, corpus_path):\n",
    "    chunk_id, seq_id = query_id.split(\"_\")\n",
    "    base_path = corpus_path\n",
    "    jsonl_file = os.path.join(base_path, \"chunk_{}.jsonl\".format(chunk_id))\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        # directly go the line\n",
    "        line = f.readlines()[int(seq_id)]\n",
    "        data = json.loads(line)\n",
    "        assert data[\"id\"] == query_id\n",
    "        return data\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type = \"queries/first\" # or \"train\"\n",
    "# dataset_name = \"rpwiki_en\"\n",
    "# dataset_name = \"redpajama_2b\"\n",
    "dataset_name = \"c4_news\"\n",
    "corpus_path = f\"/home/aiops/zhuty/ret_pretraining_data/id_added/{dataset_name}/{type}\"\n",
    "get_content(\"10_0\", corpus_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "type = \"train\" # or \"train\"\n",
    "# dataset_name = \"redpajama_2b\"\n",
    "# dataset_name = \"redpajama_20b\"\n",
    "# dataset_name = \"rpwiki_en\"\n",
    "dataset_name = \"c4_news\"\n",
    "corpus_path = f\"/home/aiops/zhuty/ret_pretraining_data/id_added/{dataset_name}/{type}\"\n",
    "get_content(\"365_10689\", corpus_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# if the newly added queries are not empty, then we need to rerun the search for them\n",
    "\n",
    "import json\n",
    "\n",
    "to_write_data = []\n",
    "not_searched_query_ids = sorted(missing_queries)\n",
    "for docid in tqdm.tqdm(not_searched_query_ids, total=len(not_searched_query_ids)):\n",
    "    chunk_id, seq_id = docid.split(\"_\")\n",
    "    base_path = \"/home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/queries\"\n",
    "    jsonl_file = os.path.join(base_path, \"chunk_{}.jsonl\".format(chunk_id))\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        # directly go the line\n",
    "        line = f.readlines()[int(seq_id)]\n",
    "        data = json.loads(line)\n",
    "        assert data[\"id\"] == docid\n",
    "\n",
    "        if len(data['title']) > 1500:\n",
    "            print(\"problematic docid\", docid)\n",
    "            print(data['title'])\n",
    "            print(len(data['title'].split()))\n",
    "            data['title'] = data['title'][-1500:]\n",
    "        to_write_data.append(data)\n",
    "\n",
    "        # a slower version\n",
    "        # for line in f:\n",
    "        #     line = line.strip()\n",
    "        #     if line:\n",
    "        #         data = json.loads(line)\n",
    "        #         if data[\"id\"] == docid:\n",
    "        #             # print(data)\n",
    "        #             # print(len(data['title'].split()))\n",
    "        #             to_write_data.append(data)\n",
    "        #             break\n",
    "# write to jsonl\n",
    "with open(os.path.join(base_path, \"chunk_missing_queries.jsonl\"), \"w\") as f:\n",
    "    for data in to_write_data:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the distribution of the scores of the top1 neighbors, compared to the scores of other neighbors in top 10\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "top_1_scores = [min(300, docs[0]['score']) for docs in result_dict.values()] # add min to avoid outliers\n",
    "others_scores = [min(300, docs[1]['score']) for docs in result_dict.values() if len(docs) > 1]\n",
    "plt.hist(top_1_scores, bins=100, alpha=0.5, label='top1')\n",
    "plt.hist(others_scores, bins=100, alpha=0.5, label='others')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Distribution of the scores of the top1 neighbors, compared to the scores of top2 neighbors\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sorted(top_1_scores, reverse=True)[:1000000][-10:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 2: analyze the traversed path results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path_files = 'Saving result to /home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/traversal_paths/result_path_adj_lst_top_100_all_degree_min_degree_selection_20240103_130057.json'\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def find_latest_file(directory, prefix, directed):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(directory)\n",
    "    if directed:\n",
    "        files = [x for x in files if 'undirected' not in x ]\n",
    "    else:\n",
    "        files = [x for x in files if 'undirected'  in x ]\n",
    "\n",
    "    # Filter files based on the prefix and extract timestamps\n",
    "    timestamped_files = []\n",
    "    for file in files:\n",
    "        if file.startswith(prefix):\n",
    "            match = re.search(r'(\\d{8}_\\d{6})', file)\n",
    "            if match:\n",
    "                timestamp = match.group(1)\n",
    "                timestamped_files.append((file, timestamp))\n",
    "\n",
    "    # Check if there are any matched files\n",
    "    if not timestamped_files:\n",
    "        return None\n",
    "\n",
    "    # Convert timestamps to datetime objects and find the latest file\n",
    "    timestamped_files.sort(key=lambda x: datetime.strptime(x[1], '%Y%m%d_%H%M%S'), reverse=True)\n",
    "    return timestamped_files[0][0]\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def get_path_length_statistics(paths):\n",
    "    # print the length of the paths\n",
    "    print(\"Number of paths:\", len(paths))\n",
    "    path_lengths = [len(path) for path in paths]\n",
    "    print(Counter(path_lengths))\n",
    "    # get statistics of the path lengths\n",
    "    print(\"Average path length:\", sum(path_lengths) / len(path_lengths))\n",
    "    print(\"Max path length:\", max(path_lengths))\n",
    "    print(\"Min path length:\", min(path_lengths))\n",
    "    print(\"std path length:\", np.std(path_lengths))\n",
    "    stats_dict = {\n",
    "        \"num_paths\": len(paths),\n",
    "        \"avg_path_length\": sum(path_lengths) / len(path_lengths),\n",
    "        \"max_path_length\": max(path_lengths),\n",
    "        \"min_path_length\": min(path_lengths),\n",
    "        \"std_path_length\": np.std(path_lengths)\n",
    "    }\n",
    "    return stats_dict\n",
    "\n",
    "\n",
    "\n",
    "row_lst = []\n",
    "paths_dir = \"/home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/traversal_paths\"\n",
    "# for k in 1 3 5 10 20 ; do\n",
    "for k in [1,3,5,10,20,100]:\n",
    "  for node_selection in [\"random\", \"min_degree\", \"max_degree\"]:\n",
    "    for degree_measure in [\"in\" ,\"out\" ,\"all\" ]:\n",
    "        for directed in [True, False]:\n",
    "            latest_path_file = find_latest_file(directory = paths_dir,\n",
    "                                                prefix = f\"result_path_adj_lst_top_{k}_{degree_measure}_degree_{node_selection}_selection\",\n",
    "                                                directed=directed)\n",
    "            print(latest_path_file)\n",
    "            paths = json.load(open(os.path.join(paths_dir, latest_path_file)),)\n",
    "            stats_dict = get_path_length_statistics(paths)\n",
    "            stats_dict['name'] = latest_path_file\n",
    "            row_lst.append(stats_dict)\n",
    "\n",
    "# create a dataframe\n",
    "df = pd.DataFrame(row_lst)\n",
    "print(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the average path length for each name in the df\n",
    "name_avg_length = zip(df['name'], df['avg_path_length'])\n",
    "name_avg_length = sorted(name_avg_length, key=lambda x: x[1])\n",
    "for name, avg_length in name_avg_length:\n",
    "    print(name, avg_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adj_lst = json.load(open(\"/home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/adj_lists/adj_lst_top_1.json\", \"r\"))\n",
    "# adj_lst = json.load(open(\"/home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/adj_lists/adj_lst_top_3.json\", \"r\"))\n",
    "# adj_lst = json.load(open(\"/home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/adj_lists/adj_lst_top_5.json\", \"r\"))\n",
    "# path_file = \"/home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/traversal_paths/result_path_adj_lst_top_1_20240103_074215.json\"\n",
    "# path_file = \"/home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/traversal_paths/result_path_adj_lst_top_3_20240103_081822.json\"\n",
    "path_file = \"/home/aiops/zhuty/ret_pretraining_data/redpajama_2b_id_added/traversal_paths/result_path_adj_lst_top_5_20240103_082740.json\"\n",
    "path_file = os.path.join(\"/home/aiops/zhuty/ret_pretraining_data/id_added/c4_news/traversal_paths/dense_first\",\n",
    "                         \"result_path_adj_lists_top10_all_degree_random_selection_undirected_20240117_172931.json\")\n",
    "paths = json.load(open(path_file, \"r\"))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# analyze the in-degree of the nodes in the adjacency list\n",
    "in_degree = defaultdict(int)\n",
    "for query_id, neighbors in tqdm.tqdm(adj_lst.items()):\n",
    "    for neighbor in neighbors:\n",
    "        if neighbor[0] not in in_degree:\n",
    "            in_degree[neighbor[0]] = 0\n",
    "        in_degree[neighbor[0]] += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "flattened_path = [item for sublist in paths for item in sublist]\n",
    "assert len(flattened_path) == 890000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(flattened_path.index('10_1832'))\n",
    "print(flattened_path.index('87_1340'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the sample paths\n",
    "start_idx = 23998\n",
    "for path in paths[start_idx:start_idx+10]:\n",
    "    print(path, adj_lst[path[0]], in_degree[path[0]], len(adj_lst[path[0]]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paths[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(paths[6])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paths[-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the distribution of length of paths\n",
    "from collections import Counter\n",
    "path_lengths = [len(path) for path in paths]\n",
    "print(Counter(path_lengths))\n",
    "\n",
    "# plot the distribution of the path lengths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.hist(path_lengths, bins=100)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Testing whether we should merge"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "orig_paths= json.load(open(\"/home/aiops/zhuty/ret_pretraining_data/id_added/redpajama_2b/traversal_paths/dense/result_path_adj_lst_top_100_all_degree_min_degree_selection_undirected_20240115_132632.json\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_jsonl_files(jsonl_dir):\n",
    "    adj_list = {}\n",
    "    for file_name in tqdm(os.listdir(jsonl_dir)):\n",
    "        file_path = os.path.join(jsonl_dir, file_name)\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line)\n",
    "                adj_list[line['query_id']] = line['docs']\n",
    "    print(\"Read\", len(adj_list), \"queries from\", jsonl_dir)\n",
    "    print(\"Read {} files from {}\".format(len(os.listdir(jsonl_dir)), jsonl_dir))\n",
    "    return adj_list\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adj_lst = read_jsonl_files(\"/home/aiops/zhuty/ret_pretraining_data/id_added/c4_news/dense_search_results/first/adj_lists\")\n",
    "adj_lst = json.load(open(\"/home/aiops/zhuty/ret_pretraining_data/id_added/redpajama_2b/adj_lists/dense/adj_lst_top_100.json\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adj_lst['0_0'][:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(adj_lst)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge(cluster2docs, doc2cluster, cluster_size = 21, top_k = 10):\n",
    "\n",
    "    # data_stats(self.cluster2docs)\n",
    "\n",
    "    merged_clusters_num = 0\n",
    "    for cluster, cluster_docs in tqdm(cluster2docs.copy().items()):\n",
    "        if len(cluster_docs) < cluster_size:\n",
    "            merged_clusters_num += 1\n",
    "            # print(merged_clusters_num)\n",
    "            for doc in cluster_docs:\n",
    "                # knns, relative_id = self.knns, doc\n",
    "                # top1k, top1k_cluster = self.output_first_doc_knn_not_in_the_cluster(knns[relative_id, :], cluster)\n",
    "                if doc not in adj_lst:\n",
    "                    neighbor = None\n",
    "                    neighbor_cluster = None\n",
    "                else:\n",
    "                    for neighbor,score in adj_lst[doc][:top_k]:\n",
    "                        # find the first neighbor that is not in the same cluster\n",
    "                        if doc2cluster[neighbor] == cluster:\n",
    "                            continue\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                    neighbor_cluster = doc2cluster[neighbor]\n",
    "                # bp()\n",
    "                k_cluster_docs = cluster2docs[neighbor_cluster]\n",
    "                # bp()\n",
    "                # add k to doc\n",
    "                # k_cluster_docs.append(k)\n",
    "                if neighbor is None:\n",
    "                    k_cluster_docs.append(doc)\n",
    "                else:\n",
    "                    k_cluster_docs.insert(k_cluster_docs.index(neighbor), doc)\n",
    "\n",
    "                # update the cluster\n",
    "                cluster2docs[neighbor_cluster] = k_cluster_docs\n",
    "                doc2cluster[doc] = neighbor_cluster\n",
    "            del cluster2docs[cluster]\n",
    "    print(merged_clusters_num)\n",
    "    return cluster2docs, doc2cluster"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note: I think this is slightly problematic\n",
    "Suppose that initially we have A in C21, C22 has B, C and D.\n",
    "Now if A joins C22, and the next time we go to C22, we would not have \"cluster_docs\" including A. Therefore it will get deleted.\n",
    "Issue solved, it wouln't , because it is a shallow copy. See the copy() function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "cluster2docs = defaultdict(list)\n",
    "doc2cluster = {}\n",
    "for i, docs in enumerate(orig_paths):\n",
    "    cluster2docs[i] = docs\n",
    "    for doc in docs:\n",
    "        doc2cluster[doc] = i"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(cluster2docs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(orig_paths[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "a = defaultdict(list)\n",
    "a[None]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_cluster2docs, new_doc2cluster = merge(cluster2docs, doc2cluster, cluster_size=21)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(new_doc2cluster)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(new_cluster2docs[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# conver to path file\n",
    "new_paths = []\n",
    "for cluster, docs in new_cluster2docs.items():\n",
    "    new_paths.append(docs)\n",
    "print(len(new_paths))\n",
    "\n",
    "json.dump(new_paths, open(\"/home/aiops/zhuty/ret_pretraining_data/id_added/redpajama_2b/traversal_paths/dense/result_path_adj_lst_top_100_all_degree_min_degree_selection_undirected_20240115_132632.json.merged\", \"w\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(new_cluster2docs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(cluster2docs[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(new_cluster2docs[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "for docid, cluster in new_doc2cluster.items():\n",
    "    print(\"New cluster:\", cluster, \"Old cluster:\", doc2cluster[docid])\n",
    "    count += 1\n",
    "    if count > 1000:\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get number of clusters and average size\n",
    "cluster_size = []\n",
    "for cluster, docs in new_cluster2docs.items():\n",
    "    cluster_size.append(len(docs))\n",
    "print(\"Number of clusters:\", len(new_cluster2docs))\n",
    "print(\"Average cluster size:\", sum(cluster_size) / len(cluster_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc2cluster['10_0']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the distribution of the cluster size\n",
    "from collections import Counter\n",
    "print(Counter(cluster_size))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max(cluster_size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.chdir(\"/home/aiops/zhuty/tinyllama/processing/graphs\")\n",
    "from utils import get_path_stats\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "basic_dir = \"/home/aiops/zhuty/ret_pretraining_data/id_added/c4_news/traversal_paths/dense_first\"\n",
    "path_file = \"result_path_adj_lists_top100_max21_all_degree_random_selection_undirected_20240119_033720.json\"\n",
    "path_file = os.path.join(basic_dir, path_file)\n",
    "paths = json.load(open(path_file, \"r\" ))\n",
    "get_path_stats(paths)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Number of paths: 12498589\n",
    "Average length: 1.0825469979051234 standard deviation: 0.9460474568199976\n",
    "Maximum length: 21\n",
    "Number of paths with length 1: 12253662\n",
    "% of paths with length 1: 98.04036279615242 %\n",
    "Top 10 paths length: [21, 21, 21, 21, 21, 21, 21, 21, 21, 21]\n",
    "Bottom 10 paths length: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synpre_env",
   "language": "python",
   "display_name": "synpre_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
