{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "# tasks = [\n",
    "#     \"agieval\", \"boolq\", \"hellaswag\", \"mathqa\", \"mrpc\", \"piqa\", \"qnli\", \"rte\", \"social_iqa\", \"winogrande\",\n",
    "#     \"arc_challenge\", \"copa\", \"lambada_standard\",  \"multirc\", \"pubmedqa\", \"qqp\", \"sciq\", \"sst2\", \"wnli\",\n",
    "#     \"arc_easy\", \"gsm8k\", \"logiqa\", \"mnli\", \"openbookqa\", \"qasper\", \"race\", \"scrolls\", \"wikitext\", \"wsc\"\n",
    "# ]\n",
    "tasks = {'group1': ['wikitext', 'lambada_standard'],\n",
    "'group2': ['triviaqa', 'nq_open', 'webqs', 'hellaswag', 'squadv2'],\n",
    "'group3': ['squadv2']}\n",
    "tasks['all'] = tasks['group1'] + tasks['group2'] + tasks['group3']\n",
    "#\n",
    "tasks = tasks['group1']\n",
    "# tasks = ['sst2']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "step_to_versions = {('1b', 'iter-600000-ckpt-step-75000_hf'):  ['cc_merged_v2_8k', 'cc_merged_v1_8k', 'intramask_cc_8k', 'intramask_cc_merged_v2_8k', 'adamask_cc_merged_v2_8k', 'cc_merged_v2_8k_intracccont','cc_8k',],\n",
    "                    ('1b', 'iter-480000-ckpt-step-60000_hf'):  ['cc_merged_v2_8k', 'intramask_cc_8k', 'intramask_cc_merged_v2_8k', 'adamask_cc_merged_v2_8k', 'cc_merged_v2_8k_intracccont','cc_8k',],\n",
    "                    ('1b', 'iter-380000-ckpt-step-47500_hf'):  ['cc_merged_v2_8k', 'intramask_cc_8k', 'intramask_cc_merged_v2_8k', 'adamask_cc_merged_v2_8k', 'cc_merged_v2_8k_intracccont','cc_8k',],\n",
    "                    }"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "nshot = 10\n",
    "size='1b'\n",
    "# iter_name = 'iter-160000-ckpt-step-40000_hf'\n",
    "# iter_name = 'iter-480000-ckpt-step-60000_hf'\n",
    "iter_name = 'iter-600000-ckpt-step-75000_hf'\n",
    "# iter_name = 'iter-380000-ckpt-step-47500_hf'\n",
    "for task_name in tasks:\n",
    "    task_performance = {}\n",
    "    # for ds_version in ['cc', 'cc_merged_v1', 'cc_merged_v2', 'cc_merged_v3', 'intramask_cc']:\n",
    "    # for ds_version in ['cc_merged_v2_8k', 'intramask_cc_8k', 'intramask_cc_merged_v2_8k', 'adamask_cc_merged_v2_8k', 'cc_merged_v2_8k_intrav2cont','cc_8k',]:\n",
    "    # for ds_version in ['cc_merged_v2_8k', 'cc_merged_v1_8k', 'intramask_cc_8k', 'adamask_cc_merged_v2_8k', 'cc_merged_v2_8k_intracccont','cc_8k',]: #\n",
    "    # intramask_cc_merged_v2_8k\n",
    "    # for ds_version in ['baseline']:\n",
    "    for ds_version in ['BM25Chunk', \"UniChunk\", \"MixChunk\", \"IntraDoc\"]:\n",
    "\n",
    "        # BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/home/aiops/zhuty/tinyllama/out/tiny_LLaMA_1b_8k_{ds_version}_8k/iter-380000-ckpt-step-47500_hf\"\n",
    "        # BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/home/aiops/zhuty/tinyllama/out/tiny_LLaMA_{size}_8k_{ds_version}/{iter_name}\"\n",
    "        BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/tyzhu/tiny_LLaMA_{size}_8k_{ds_version}_{iter_name}\"\n",
    "        BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/yuzhaouoe/{ds_version}\"\n",
    "    #\n",
    "    #     BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "        # BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/home/aiops/zhuty/lm_indexer_data/tyzhu/flan_max_300_added_tyzhu_tiny_LLaMA_1b_8k_{ds_version}_8k_iter-380000-ckpt-step-47500_hf/checkpoint-12387\"\n",
    "#         result_file_path = os.path.join(BASE_PATH, f\"{task_name}\", f\"results_{nshot}.json\")\n",
    "        # result_file_path = os.path.join(BASE_PATH, f\"{task_name}\", f\"{nshot}\", f\"tyzhu__tiny_LLaMA_{size}_8k_{ds_version}_{iter_name}\")\n",
    "        # result is a file that begins with 'result', get the latest result\n",
    "        # result_files = [f for f in os.listdir(result_file_path) if f.startswith('result')]\n",
    "        # result_files.sort()\n",
    "        # result_file_path = os.path.join(result_file_path, result_files[-1])\n",
    "        result_file_path = os.path.join(BASE_PATH, f\"{task_name}\", f\"{nshot}\", f\"results.json\")\n",
    "        if not os.path.exists(result_file_path):\n",
    "            print(f\"File not found\", task_name, ds_version)\n",
    "            continue\n",
    "        result = json.load(open(result_file_path))\n",
    "        # print(result)\n",
    "        assert result['n-shot'][task_name] == nshot, \"n-shot not match, got %d\" % result['n-shot'][task_name] + \" expected %d\" % nshot + \"in task %s\" % task_name + \"of model %s\" % ds_version\n",
    "        result = result['results'][task_name]\n",
    "        task_performance[ds_version] = result\n",
    "        # break\n",
    "    # make the dataframe\n",
    "    print(\"Task:\", task_name)\n",
    "    if len(task_performance) == 0:\n",
    "        continue\n",
    "    df = pd.DataFrame(task_performance)\n",
    "    df = pd.concat([df.loc[['alias']], df.drop('alias')])\n",
    "    all_dfs.append(df)\n",
    "    print(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_df = pd.concat(all_dfs, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Compare the same model over different steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_dfs = []\n",
    "nshot = 0\n",
    "size='360M'\n",
    "for task_name in tasks:\n",
    "    task_performance = {}\n",
    "    # for ds_version in ['cc_merged_v2_8k', 'intramask_cc_8k', 'intramask_cc_merged_v2_8k', 'adamask_cc_merged_v2_8k', 'cc_merged_v2_8k_intrav2cont','cc_8k',]:\n",
    "    ds_version = 'cc_merged_v2_8k_intrav2cont'\n",
    "    for step in range(27500, 67500 + 2500, 2500):\n",
    "    # for step in range(17500, 80000 + 2500, 2500):\n",
    "        # BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/home/aiops/zhuty/tinyllama/out/tiny_LLaMA_1b_8k_{ds_version}_8k/iter-380000-ckpt-step-47500_hf\"\n",
    "        BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/home/aiops/zhuty/tinyllama/out/tiny_LLaMA_{size}_8k_{ds_version}/iter-{step*4:06}-ckpt-step-{step}_hf\"\n",
    "        iter_name = f\"iter-{step*4:06}-ckpt-step-{step}_hf\"\n",
    "\n",
    "        # BASE_PATH = f\"/home/aiops/zhuty/tinyllama/scripts/lm_harness_eval/out/home/aiops/zhuty/lm_indexer_data/tyzhu/flan_max_300_added_tyzhu_tiny_LLaMA_1b_8k_{ds_version}_8k_iter-380000-ckpt-step-47500_hf/checkpoint-12387\"\n",
    "        # result_file_path = os.path.join(BASE_PATH, f\"{task_name}\", f\"results_{nshot}.json\")\n",
    "        # result_file_path = os.path.join(BASE_PATH, f\"{task_name}\", f\"{nshot}\", f\"results.json\")\n",
    "        result_file_path = os.path.join(BASE_PATH, f\"{task_name}\", f\"{nshot}\", f\"__home__aiops__zhuty__tinyllama__out__tiny_LLaMA_{size}_8k_{ds_version}__{iter_name}\")\n",
    "        # result is a file that begins with 'result', get the latest result\n",
    "        result_files = [f for f in os.listdir(result_file_path) if f.startswith('result')]\n",
    "        result_files.sort()\n",
    "        result_file_path = os.path.join(result_file_path, result_files[-1])\n",
    "\n",
    "\n",
    "        if not os.path.exists(result_file_path):\n",
    "            print(f\"File not found\", task_name, result_file_path)\n",
    "            continue\n",
    "        result = json.load(open(result_file_path))\n",
    "        # print(result)\n",
    "        assert result['n-shot'][task_name] == nshot, \"n-shot not match, got %d\" % result['n-shot'][task_name] + \" expected %d\" % nshot + \"in task %s\" % task_name + \"of model %s\" % ds_version\n",
    "        result = result['results'][task_name]\n",
    "        task_performance[ds_version] = result\n",
    "        # break\n",
    "    # make the dataframe\n",
    "        print(\"Task:\", task_name)\n",
    "        if len(task_performance) == 0:\n",
    "            continue\n",
    "        df = pd.DataFrame(task_performance)\n",
    "        df = pd.concat([df.loc[['alias']], df.drop('alias')])\n",
    "        df = df.transpose()\n",
    "        df['step'] = step\n",
    "        all_dfs.append(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "concat_df = pd.concat(all_dfs, axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "concat_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the word perplexity against the step\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(concat_df['step'], concat_df['word_perplexity,none'], label=f\"{task_name}-{ds_version}\")\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('word_perplexity')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing the tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = LlamaForConditionalGeneration.from_pretrained(\"tyzhu/tiny_LLaMA_1b_8k_cc_merged_v3_8k\")\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", token = json.load(open('/home/aiops/zhuty/hf_token.json')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.decode([13])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer.encode(\"How am I doing? \\n\\n \"\n",
    "                 \"I am doing well\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "commands = []\n",
    "# for step in range(17500, 80000+2500, 2500):\n",
    "for step in range(27500, 67500 + 2500, 2500):\n",
    "    # commands.append(f\" bash convert_to_hf_general.sh /home/aiops/zhuty/tinyllama/out/tiny_LLaMA_360M_8k_cc_merged_v2_8k_intrav2cont  iter-{step*4:06}-ckpt-step-{step}\")\n",
    "    commands.append(f\" bash eval.sh  wikitext /home/aiops/zhuty/tinyllama/out/tiny_LLaMA_360M_8k_cc_merged_v2_8k_intrav2cont/iter-{step*4:06}-ckpt-step-{step}_hf 0\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\";\".join(commands))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "# Initialize the API client\n",
    "api = HfApi()\n",
    "api_token = json.load(open(\"/home/aiops/zhuty/hf_token.json\"))\n",
    "\n",
    "# List models with the specified name\n",
    "models = api.list_models(author=\"tyzhu\", use_auth_token=api_token)\n",
    "for model in models:\n",
    "    model_id = model.modelId\n",
    "    print(f\"Making model '{model_id}' private\")\n",
    "    api.update_repo_visibility(repo_id=model_id, private=True, token=api_token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import json\n",
    "for step in range(5000, 80000, 5000):\n",
    "    try:\n",
    "        HfApi().model_info(f\"tyzhu/tiny_LLaMA_1b_8k_cc_8k_iter-{step*8:06}-ckpt-step-{step}_hf\", token =json.load(open('/home/aiops/zhuty/hf_token.json')))\n",
    "    except:\n",
    "        print(f\"tyzhu/tiny_LLaMA_1b_8k_cc_8k_iter-{step*8:06}-ckpt-step-{step}_hf\" + \" not found\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synpre_env",
   "language": "python",
   "display_name": "synpre_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
