{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def remove_underline(x):\n",
    "    return x.replace(\"_\", \"\")\n",
    "\n",
    "def normalise_pred(pred):\n",
    "    return pred.strip().split(\"\\n\")[0].strip()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BASE_PATH = '/home/aiops/zhuty/tinyllama/scripts/fewshot_eval/outputs/'\n",
    "commands = []\n",
    "data = []\n",
    "\n",
    "\n",
    "high_priority=False\n",
    "if high_priority:\n",
    "    high_string = \"-p high\"\n",
    "else:\n",
    "    high_string = \"\"\n",
    "# size=\"360M\"\n",
    "# iter_name = \"iter-160000-ckpt-step-40000_hf\"\n",
    "#iter_name = \"iter-110000-ckpt-step-27500_hf\"\n",
    "# iter_name = \"iter-260000-ckpt-step-65000_hf\"\n",
    "\n",
    "size=\"1b\"\n",
    "\n",
    "# Define the list of iter_name and model_names\n",
    "iterations = [\n",
    "    {\n",
    "        \"iter_name\": \"iter-480000-ckpt-step-60000_hf\",\n",
    "        \"model_names\": ['cc_merged_v2_8k', 'intramask_cc_8k', 'cc_merged_v2_8k_intracccont', 'cc_8k', 'adamask_cc_merged_v2_8k', 'intramask_cc_merged_v2_8k']\n",
    "    },\n",
    "    {\n",
    "        \"iter_name\": \"iter-380000-ckpt-step-47500_hf\",\n",
    "        \"model_names\": ['cc_merged_v2_8k', 'intramask_cc_8k', 'cc_merged_v2_8k_intracccont', 'cc_8k', 'adamask_cc_merged_v2_8k', 'intramask_cc_merged_v2_8k']\n",
    "    },\n",
    "    {\n",
    "        \"iter_name\": \"iter-600000-ckpt-step-75000_hf\",\n",
    "        \"model_names\": ['cc_merged_v2_8k', 'intramask_cc_8k', 'cc_merged_v2_8k_intracccont', 'cc_8k', 'cc_merged_v1_8k', 'adamask_cc_merged_v2_8k', 'intramask_cc_merged_v2_8k']\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# model_names =  ['BM25Chunk', 'IntraDoc', 'UniChunk', 'MixChunk']\n",
    "# 'intramask_cc_merged_v2_8k']\n",
    "# model_names = ['intramask_cc_8k', 'intramask_cc_merged_v2_8k', 'cc_merged_v2_8k_intrav2cont','cc_8k', 'adamask_cc_merged_v2_8k',  'cc_merged_v2_8k',]\n",
    "# model_names = ['cc_merged_v2_8k', 'intramask_cc_8k', 'cc_merged_v1_8k', 'adamask_cc_merged_v2_8k', 'cc_merged_v2_8k_intracccont','cc_8k',]\n",
    "# model_names = ['baseline']\n",
    "\n",
    "# TASK_CLASS, shot_nums, rag_num_docs_lst =\"obqa\", [8, 16], [\"\"]\n",
    "# TASK_CLASS, shot_nums, rag_num_docs_lst =\"obqa\", [4], [\"\"]\n",
    "# TASK_CLASS, shot_nums, rag_num_docs_lst =\"icl\", [24,48], [\"\"]\n",
    "# TASK_CLASS, shot_nums, rag_num_docs_lst =\"memtrap\", [0], [\"\"]\n",
    "# TASK_CLASS, shot_nums, rag_num_docs_lst =\"cbqa\", [12,24], [\"\"]\n",
    "# TASK_CLASS, shot_nums, rag_num_docs_lst =\"obqa_rag\", [2], [1,3,5,10]\n",
    "#\n",
    "\n",
    "# Define the list of TASK_CLASS, shot_nums, and rag_num_docs_lst\n",
    "tasks = [\n",
    "    {\n",
    "        \"TASK_CLASS\": \"obqa\",\n",
    "        \"shot_nums\": [4],\n",
    "        \"rag_num_docs_lst\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "        \"TASK_CLASS\": \"icl\",\n",
    "        \"shot_nums\": [24, 48],\n",
    "        \"rag_num_docs_lst\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "        \"TASK_CLASS\": \"memtrap\",\n",
    "        \"shot_nums\": [0],\n",
    "        \"rag_num_docs_lst\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "        \"TASK_CLASS\": \"cbqa\",\n",
    "        \"shot_nums\": [12, 24],\n",
    "        \"rag_num_docs_lst\": [\"\"]\n",
    "    },\n",
    "    {\n",
    "        \"TASK_CLASS\": \"obqa_rag\",\n",
    "        \"shot_nums\": [2],\n",
    "        \"rag_num_docs_lst\": [1, 3, 5, 10]\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "class_to_tasks_mapping = {\"icl\":  ['tweet_hate' ,'tweet_offensive', 'agnews' ,'amazon' ,'dbpedia' ,'sst2','yelp' ],\n",
    "                          \"obqa\": [ \"squad\",\"hotpotqa\"],\n",
    "                          \"obqa_rag\": [\"nq_obqa\", \"tq_obqa\"],\n",
    "                          \"cbqa\": [\"tq\", \"nq\"],\n",
    "                          \"memtrap\" :[\"memtrap\"]\n",
    "                          }\n",
    "\n",
    "class_to_seed_mapping = {\"icl\": range(42, 58),\n",
    "                         \"obqa\": range(42, 46+1),\n",
    "                         \"cbqa\": range(42, 46 + 1),\n",
    "                            \"obqa_rag\": range(42, 46 + 1),\n",
    "                         \"memtrap\":[43]\n",
    "                         }\n",
    "\n",
    "# Loop through each set of iter_name and model_names\n",
    "for iteration in iterations:\n",
    "    iter_name = iteration[\"iter_name\"]\n",
    "    model_names = iteration[\"model_names\"]\n",
    "\n",
    "    print(f\"Processing {iter_name} with models:\")\n",
    "    for model_name in model_names:\n",
    "        print(f\" - {model_name}\")\n",
    "    # Loop through each set of TASK_CLASS, shot_nums, and rag_num_docs_lst\n",
    "    for task in tasks:\n",
    "        TASK_CLASS = task[\"TASK_CLASS\"]\n",
    "        shot_nums = task[\"shot_nums\"]\n",
    "        rag_num_docs_lst = task[\"rag_num_docs_lst\"]\n",
    "\n",
    "\n",
    "\n",
    "        for task_name in  class_to_tasks_mapping[TASK_CLASS]:\n",
    "            # for shot_num in [1, 3, 5, 10, 20, 30, 48]:\n",
    "            # for shot_num in [3,5,10,20,30,48]:\n",
    "            # for shot_num in [24, 48]:\n",
    "            for shot_num in shot_nums:\n",
    "                for rag_num_docs in rag_num_docs_lst:\n",
    "                    if task_name == \"yelp\" and shot_num == 48:\n",
    "                       continue\n",
    "                    for model_name in model_names:\n",
    "                    # for step in range(5000, 80000, 5000):\n",
    "                    # for step in [75000]:\n",
    "                        # iter_name = f'iter-{step*8:06}-ckpt-step-{step}_hf'\n",
    "                        # model_name = 'cc_merged_v2_8k'\n",
    "                        if '8k' not in model_name and 'cc' in model_name:\n",
    "                            model_name = f\"{model_name}_8k\"\n",
    "                    # for model_name in ['bstilm']:\n",
    "                        for seed_num in class_to_seed_mapping[TASK_CLASS]:\n",
    "                        # for seed_num in [42]:\n",
    "                            full_name = f\"tiny_LLaMA_{size}_8k_{model_name}-{iter_name}\"\n",
    "                            # full_name = \"TinyLlama-1.1B-step-50K-105b\"\n",
    "                            if TASK_CLASS == \"obqa_rag\":\n",
    "                                result_path = os.path.join(BASE_PATH,full_name, f'{task_name}_{shot_num}_{seed_num}_{rag_num_docs}.json')\n",
    "                            else:\n",
    "                                result_path = os.path.join(BASE_PATH,full_name, f'{task_name}_{shot_num}_{seed_num}.json')\n",
    "                            if not os.path.exists(result_path):\n",
    "                                # print(f\"File {result_path} does not exist\")\n",
    "                                # model_path = f\"/home/aiops/zhuty/tinyllama/out/tiny_LLaMA_{size}_8k_{model_name}/{iter_name}\"\n",
    "                                model_path = f\"tyzhu/tiny_LLaMA_{size}_8k_{model_name}_{iter_name}\"\n",
    "                                if model_name in ['BM25Chunk', 'IntraDoc', 'UniChunk', 'MixChunk'] :\n",
    "                                    model_path = f\"yuzhaouoe/{model_name}\"\n",
    "\n",
    "\n",
    "                                # check if config file exists\n",
    "                                assert os.path.exists(f\"{model_path}/config.json\") or 'tyzhu/' in model_path or 'yuzhao' in model_path, \"Missing config file for {}, convert it with \\n {}\".format(model_path, f\"bash convert_to_hf_general.sh /home/aiops/zhuty/tinyllama/out/tiny_LLaMA_{size}_8k_{model_name} {iter_name.strip('_hf')}\")\n",
    "\n",
    "                                if TASK_CLASS == \"obqa_rag\":\n",
    "                                    assert str(rag_num_docs) != \"\"\n",
    "                                else:\n",
    "                                    assert str(rag_num_docs) == \"\"\n",
    "                                jobname = f\"eval{size.lower()}{remove_underline(task_name)}{remove_underline(model_name)}{seed_num}\"[:40]\n",
    "                                # curr = f\"\"\" sailctl job create {jobname} -g 1  --debug --image asia-docker.pkg.dev/sail-tpu-02/git-insea-io/common/image_registry/liuqian/tinyllama:v9   --command-line-args  ' bash /home/aiops/zhuty/start.sh ; cd /home/aiops/zhuty/tinyllama/scripts/fewshot_eval/ ; bash single_task.sh {model_path} {task_name} {seed_num} {shot_num} {rag_num_docs}' {high_string} \"\"\"\n",
    "                                curr = (jobname, model_path, task_name, f\"bash single_task.sh {model_path} {task_name} {seed_num} {shot_num} {rag_num_docs} \")\n",
    "\n",
    "                                commands.append(curr)\n",
    "\n",
    "                                continue\n",
    "                            else:\n",
    "                                # print(\"Existing\", result_path)\n",
    "                                result = json.load(open(result_path, 'r'))\n",
    "                                if TASK_CLASS == \"obqa_rag\":\n",
    "                                    result['num_docs'] = rag_num_docs\n",
    "                                data.append(result)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert len(commands) == 0 , \"missing commands {}\".format( len(commands))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_bash_commands = {}\n",
    "long_task_commands = []\n",
    "SHORT_TASKS = ['tweet_hate', 'tweet_offensive', 'sst2', 'memtrap']\n",
    "for job_name, model_path, task_name,  bash_command in commands:\n",
    "    job_name = job_name.replace(\"_\", \"\").replace(\"-\", \"\").lower()[:39]\n",
    "    if task_name not in SHORT_TASKS:\n",
    "        curr = f\"\"\"sailctl job create {job_name} -g 1  --debug --image asia-docker.pkg.dev/sail-tpu-02/git-insea-io/common/image_registry/liuqian/tinyllama:v9   --command-line-args  ' bash /home/aiops/zhuty/start.sh ; cd /home/aiops/zhuty/tinyllama/scripts/fewshot_eval/ ; {bash_command} ' {high_string} \"\"\"\n",
    "        long_task_commands.append(curr)\n",
    "        continue\n",
    "    if (model_path, task_name) not in model_bash_commands:\n",
    "        model_bash_commands[(model_path, task_name)] = []\n",
    "    model_bash_commands[(model_path, task_name)].append(bash_command)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(long_task_commands))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(len([x  for x in long_task_commands if 'intracc' in x]))\n",
    "print(\" ; sleep 1; \".join([x  for x in long_task_commands[:] ]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raw_commands = []\n",
    "for model_name in ['BM25Chunk', 'IntraDoc', 'UniChunk', 'MixChunk']:\n",
    "    raw_commands.append(f'bash single_task.sh yuzhaouoe/{model_name} memtrap 43 0 ')\n",
    "print(\" ;\".join(raw_commands))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "short_task_commands = []\n",
    "for (model_path, task_name), bash_commands in model_bash_commands.items():\n",
    "    # model_short_name = model_path.split('/')[-2].split('_8k')[1]\n",
    "    model_short_name = model_path.split('/')[-1]\n",
    "    jobname = f\"{task_name}{model_short_name}\".replace(\"_\", \"\").replace(\"-\", \"\").lower()[:39]\n",
    "    curr = f\"\"\"sailctl job create {jobname} -g 1  --debug --image asia-docker.pkg.dev/sail-tpu-02/git-insea-io/common/image_registry/liuqian/tinyllama:v9   --command-line-args  ' bash /home/aiops/zhuty/start.sh ; cd /home/aiops/zhuty/tinyllama/scripts/fewshot_eval/ ; {\" ; sleep 2; \".join(bash_commands)}' {high_string}\"\"\"\n",
    "    short_task_commands.append(curr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(short_task_commands)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\" ; sleep 1; \".join([x for x in short_task_commands if 'g' in x]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ensure that no score is zero\n",
    "assert len(df[df['score'] == 0]) == 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_step(x):\n",
    "    # strip the _hf\n",
    "    x = x[:-3]\n",
    "    return int(x.split('-')[-1])\n",
    "\n",
    "def extract_model_name(x):\n",
    "    \"\"\"\n",
    "    Sample input: /home/aiops/zhuty/tinyllama/out/tiny_LLaMA_1b_8k_cc_8k/iter-200000-ckpt-step-25000_hf\n",
    "    Sample output: tiny_LLaMA_1b_8k\n",
    "    Args:\n",
    "        x:\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if 'TinyLlama/TinyLlama-1.1B-step-50K-105b' in x:\n",
    "        return 'tinyllama-1.1b-50k-105b'\n",
    "    if '/home/aiops/zhuty' in x:\n",
    "        folder_name = x.split('/')[-2] # tiny_LLaMA_1b_8k_cc_8k\n",
    "        return (\"_\").join(folder_name.split('_')[:4]) # tiny_LLaMA_1b_8k\n",
    "    else:\n",
    "        # sample model name: tyzhu/tiny_LLaMA_120M_8k_cc_8k_iter-240000-ckpt-step-60000_hf\n",
    "        return x.strip(\"tyzhu/\").split(\"_cc\")[0]\n",
    "\n",
    "def extract_pretrain_dataset(x):\n",
    "    \"\"\"\n",
    "    Sample input: /home/aiops/zhuty/tinyllama/out/tiny_LLaMA_1b_8k_cc_8k/iter-200000-ckpt-step-25000_hf\n",
    "    Sample output: cc_8k\n",
    "    Args:\n",
    "        x:\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    if '/home/aiops/zhuty' in x:\n",
    "        folder_name = x.split('/')[-2] # tiny_LLaMA_1b_8k_cc_8k\n",
    "        return (\"_\").join(folder_name.split('_')[4:]) # cc_8k\n",
    "    else:\n",
    "        # sample model name: tyzhu/tiny_LLaMA_120M_8k_cc_8k_iter-240000-ckpt-step-60000_hf\n",
    "        return x.strip(\"tyzhu/\").split(\"_8k_\")[1]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "short_task_commands = []\n",
    "raw_commands = []\n",
    "for (model_path, task_name), bash_commands in model_bash_commands.items():\n",
    "    # model_short_name = model_path.split('/')[-2].split('_8k')[1]\n",
    "    model_short_name = model_path.split('/')[-1]\n",
    "    jobname = f\"{task_name}{model_short_name}\".replace(\"_\", \"\").replace(\"-\", \"\").lower()[:39]\n",
    "    curr = f\"\"\"sailctl job create {jobname} -g 1  --debug --image asia-docker.pkg.dev/sail-tpu-02/git-insea-io/common/image_registry/liuqian/tinyllama:v9   --command-line-args  ' bash /home/aiops/zhuty/start.sh ; cd /home/aiops/zhuty/tinyllama/scripts/fewshot_eval/ ; {\" ; sleep 2; \".join(bash_commands)}' {high_string}\"\"\"\n",
    "    short_task_commands.append(curr)\n",
    "    raw_commands.extend(bash_commands)\n",
    "print(\" ;\".join(raw_commands))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract model name from the path\n",
    "def extract_model_name(model_path):\n",
    "    # Using regular expression to find model names like 'tiny_LLaMA_1b_cc_merged_8k' from the path\n",
    "    match = re.search(r\"tiny_LLaMA_(\\w+)\", model_path)\n",
    "    return match.group(1) if match else \"Unknown\"\n",
    "\n",
    "# Apply the function to extract model names\n",
    "df['model_name'] = df['model'].apply(extract_model_name)\n",
    "\n",
    "# Group the data by task, n_shot, and model and calculate the mean and std deviation\n",
    "grouped_data = df.groupby(['task', 'n_shot', 'model_name'])['score'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Display the processed data\n",
    "grouped_data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "unique_tasks = grouped_data['task'].unique()\n",
    "names_to_meaning = {\"8k_cc_8k\": \"random\", \"8k_cc_merged_v1_8k\": \"dense\",\n",
    "                    \"8k_cc_merged_v2_8k\": \"bm25\", \"8k_cc_merged_v3_8k\": \"gen+bm25\",\n",
    "                    \"8k_intramask_cc_8k\": \"intradoc\"}\n",
    "# Correcting the plot to ensure error bars for standard deviation are visible\n",
    "def plot_task_data_corrected(task):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    task_data = grouped_data[grouped_data['task'] == task]\n",
    "\n",
    "    # Plot each model separately to ensure error bars are added correctly\n",
    "    for model in task_data['model_name'].unique():\n",
    "        model_data = task_data[task_data['model_name'] == model]\n",
    "        # no errorbars\n",
    "        plt.plot(model_data['n_shot'], model_data['mean'], '-o', label=names_to_meaning[model])\n",
    "        # plt.errorbar(model_data['n_shot'], model_data['mean'], yerr=model_data['std'], fmt='-o', capsize=5, label=names_to_meaning[model])\n",
    "\n",
    "    plt.title(f'Performance of Models on {task} Task')\n",
    "    plt.xlabel('Number of Shots')\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot corrected charts for each task\n",
    "for task in unique_tasks:\n",
    "    plot_task_data_corrected(task)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['step'] = df['model'].apply(extract_step)\n",
    "df['model_name'] = df['model'].apply(extract_model_name)\n",
    "df['pretrain_dataset'] = df['model'].apply(extract_pretrain_dataset)\n",
    "# df['pretrain_dataset'] = 'tllama'\n",
    "# only filter the nshot to be 24\n",
    "df = df[df['n_shot'] == 48]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming `df` is your DataFrame\n",
    "# Step 1: Group and aggregate\n",
    "grouped = df.groupby(['model_name', 'task']).agg({\n",
    "    'score': ['mean', 'std'],\n",
    "    'seed': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Step 2: Pivot the table\n",
    "# The tricky part here is dealing with multi-level column names resulting from aggregation\n",
    "grouped.columns = ['pretrain_dataset', 'task', 'score_mean', 'score_std', 'seeds']\n",
    "pivot_df = grouped.pivot_table(index='pretrain_dataset',\n",
    "                               columns='task',\n",
    "                               values=['score_mean', 'score_std', 'seeds'],\n",
    "                               aggfunc='first').reset_index()\n",
    "\n",
    "# Step 3: Rename columns for clarity\n",
    "pivot_df.columns = ['_'.join(col).strip() if col[1] else col[0] for col in pivot_df.columns.values]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `df` is your DataFrame\n",
    "# Step 1: Group and aggregate\n",
    "grouped = df.groupby(['model_name', 'task']).agg({\n",
    "    'score': [('score_mean', 'mean'), ('score_std', 'std')],\n",
    "    'seed': [('seeds', lambda x: list(x))]\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the multi-level column names\n",
    "grouped.columns = ['pretrain_dataset', 'task', 'score_mean', 'score_std', 'seeds']\n",
    "\n",
    "# Combine mean and std into a single string in the format mean(std)\n",
    "grouped['score_combined'] = grouped['score_mean'].round(2).astype(str) + \"(\" + grouped['score_std'].round(2).astype(str) + \")\"\n",
    "\n",
    "# Calculate average performance across all tasks for each pretrain_dataset\n",
    "average_performance = grouped.groupby('pretrain_dataset')['score_mean'].mean().reset_index()\n",
    "average_performance.columns = ['pretrain_dataset', 'average_performance']\n",
    "\n",
    "# Step 2: Pivot the table\n",
    "pivot_df = grouped.pivot_table(index='pretrain_dataset',\n",
    "                               columns='task',\n",
    "                               values=['score_combined', 'seeds'],\n",
    "                               aggfunc='first').reset_index()\n",
    "\n",
    "# Step 3: Rename columns for clarity\n",
    "pivot_df.columns = ['_'.join(col).strip() if col[1] else col[0] for col in pivot_df.columns.values]\n",
    "\n",
    "# Merge the average performance data\n",
    "pivot_df = pivot_df.merge(average_performance, on='pretrain_dataset', how='left')\n",
    "\n",
    "# Now, pivot_df is the desired DataFrame with an 'average_performance' column\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "average_performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# sanity check, ensure that all seed columns contain the same list\n",
    "for col in pivot_df.columns:\n",
    "    if 'seeds' in col:\n",
    "        for row in pivot_df[col]:\n",
    "            try:\n",
    "                seed_list = list((class_to_seed_mapping[TASK_CLASS]))\n",
    "                assert row== list((class_to_seed_mapping[TASK_CLASS]))\n",
    "            except:\n",
    "                print(col)\n",
    "                print(row)\n",
    "                print(seed_list)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_pivot_df = pivot_df.drop(columns=[col for col in pivot_df.columns if 'seeds' in col])\n",
    "# rename the columns to remove the 'score_combined' prefix\n",
    "cleaned_pivot_df.columns = [col.replace('score_combined_', '') for col in cleaned_pivot_df.columns]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_pivot_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# random baselines:\n",
    "# agnews: 25% ,\n",
    "# amazon: 50%\n",
    "# dbpedia: 7% ?\n",
    "# sst2: 50%\n",
    "# tweet hate: 50%\n",
    "# tweet_offensive: 50%\n",
    "# yelp: 50%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pivot_df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get all tasks\n",
    "df['task'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot the performance of 1b_8k on nq task\n",
    "import matplotlib.pyplot as plt\n",
    "INTERSTED_MODEL_NAME = 'tiny_LLaMA_1b_8k'\n",
    "TASK_NAME = 'amazon'\n",
    "df_1b_8k = df[df['model_name'] == INTERSTED_MODEL_NAME]\n",
    "df_1b_8k = df_1b_8k[df_1b_8k['task'] == TASK_NAME]\n",
    "# drop the columns that cannot be averaged\n",
    "df_1b_8k = df_1b_8k.drop(columns=['model', 'task', 'model_name'])\n",
    "# get the average over seeds, and standard deviation\n",
    "df_1b_8k = df_1b_8k.groupby(['pretrain_dataset', 'step']).agg(['mean', 'std']).reset_index()\n",
    "for dataset in df_1b_8k['pretrain_dataset'].unique():\n",
    "    df_1b_8k_dataset = df_1b_8k[df_1b_8k['pretrain_dataset'] == dataset]\n",
    "    plt.errorbar(df_1b_8k_dataset['step'], df_1b_8k_dataset['score']['mean'], yerr=df_1b_8k_dataset['score']['std'], label=dataset)\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New version: based on the inputs itself"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(commands))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\" ;\".join(commands))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = eval_callables = {\n",
    "    \"nq\": 1,\n",
    "    \"tq\": 1,\n",
    "    \"wq\": 1,\n",
    "    \"sst2\": 1,\n",
    "    \"agnews\": 1,\n",
    "    \"nq_obqa\": 1,\n",
    "    \"hotpotqa\": 1,\n",
    "    \"amazon\": 1,\n",
    "    \"dbpedia\": 1,\n",
    "    \"yelp\": 1,\n",
    "    \"tweet_hate\": 1,\n",
    "    \"tweet_offensive\": 1,\n",
    "    \"squad\": 1,\n",
    "    \"tq_obqa\": 1,\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(commands[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the output logs file\n",
    "with open('/home/aiops/zhuty/tinyllama/scripts/fewshot_eval/outputs/logs', 'r') as file:\n",
    "    data = file.readlines()\n",
    "data = [ json.loads(x) for x in data]\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Misc utils files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# copy the files\n",
    "for ds in ['cc', 'cc_merged_v1', 'cc_merged_v2', 'intramask_cc']:\n",
    "    print(f\"mkdir -p tiny_LLaMA_1b_8k_{ds}_8k\")\n",
    "    print(\n",
    "        f\"rsync -ar --progress /s3/tinyllama/out_apr24/out/tiny_LLaMA_1b_8k_{ds}_8k/iter-380000-ckpt-step-47500_hf tiny_LLaMA_1b_8k_{ds}_8k/iter-380000-ckpt-step-47500_hf\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "size = \"1b\"\n",
    "# iter_name = \"iter-480000-ckpt-step-60000_hf\"\n",
    "# iter_name = \"iter-380000-ckpt-step-47500_hf\"\n",
    "iter_name = \"iter-480000-ckpt-step-60000_hf\"\n",
    "\n",
    "model_names = ['cc_merged_v2_8k', 'intramask_cc_8k', 'cc_merged_v2_8k_intracccont', 'cc_8k', 'adamask_cc_merged_v2_8k',\n",
    "               'intramask_cc_merged_v2_8k']\n",
    "for model_name in model_names:\n",
    "    # print(f\"python upload_to_hf.py /home/aiops/zhuty/tinyllama/out/tiny_LLaMA_{size}_8k_{model_name}/{iter_name}\")\n",
    "    assert os.path.exists(f\"/home/aiops/zhuty/tinyllama/out/tiny_LLaMA_{size}_8k_{model_name}/{iter_name}/config.json\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "synpre_env",
   "language": "python",
   "display_name": "synpre_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
